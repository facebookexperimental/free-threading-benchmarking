# After making changes to this file, re-run:
#    python -m bench_runner install
# to have the changes reflected in the generated workflow files

[bases]
# List the base versions you want to compare every benchmark run to
versions = ["3.12.6", "3.13.0rc2"]
compare_to_default = ["NOGIL"]

[benchmarks]
# List any benchmarks you want to exclude from results here
excluded_benchmarks = [
    # Benchmarks where the mean execution count < 5
    "aiohttp",
    "asyncio_tcp",
    "asyncio_tcp_ssl",
    "bench_mp_pool",
    "bench_thread_pool",
    "deepcopy_reduce",
    "logging_silent",
    "pickle",
    "pickle_dict",
    "pickle_list",
    "unpack_sequence",
    "unpickle",
    "unpickle_list",
]

# Each of the "runners" section has the following values:
# os: one of 'linux', 'darwin', 'windows'
# arch: 'amd64' or 'arm64' (used for display only)
# hostname: Must match the hostname of the machine (as retrieved from
#   socket.gethostname()).
# available: True if the machine is available (we need to keep the
#   metadata around for old machines for the generated results). (Default: True)

[[runners]]

[runners.vultr]
os = "linux"
arch = "x86_64"
hostname = "pyperf"
available = true

[runners.linux]
os = "linux"
arch = "x86_64"
hostname = "pyperf"
available = true

[runners.macm4pro]
os = "darwin"
arch = "arm64"
hostname = "macmini"
available = true

[plot]
bases = ["3.12.6", "3.13.0rc2"]
versions = [[3, 13], [3, 14]]
runners = ["linux", "vultr", "macm4pro"]
names = ["linux", "linux-vultr", "macos"]
colors = ["C0", "C0", "C2"]
styles = ["-", ":", "-"]
markers = ["s", "s", "^"]

[publish_mirror]
skip = true

[notify]
notification_issue = 4
